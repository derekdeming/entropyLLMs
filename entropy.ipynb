{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: filelock in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.11.17)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.31.0\n",
      "Requirement already satisfied: torch in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.31.0\n",
    "!pip install torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT 2 WITH ATTENTION OUTPUTS ####\n",
    "\n",
    "with this we're registering forward hooks on each attention layer to capture the attention outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GPT2WithAttentionOutputs(GPT2LMHeadModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.attention_scores = []\n",
    "\n",
    "#     def reset_attention_scores(self):\n",
    "#         self.attention_scores = []\n",
    "\n",
    "#     def save_attention_scores(self, module, input, output):\n",
    "#         self.attention_scores.append(output[1]) # output[1] contains the attention probabilities\n",
    "\n",
    "#     def forward(self, input_ids, past_key_values=None, attention_mask=None):\n",
    "#         self.reset_attention_scores()\n",
    "\n",
    "#         # use hooks to capture attention outputs\n",
    "#         hooks = []\n",
    "#         for block in self.transformer.h:\n",
    "#             hook = block.attn.register_forward_hook(self.save_attention_scores)\n",
    "#             hooks.append(hook)\n",
    "#         outputs = super().forward(\n",
    "#             input_ids=input_ids,\n",
    "#             past_key_values=past_key_values,\n",
    "#             attention_mask=attention_mask,\n",
    "#             output_attentions=True,\n",
    "#             return_dict=True,\n",
    "#         )\n",
    "#         for hook in hooks:\n",
    "#             hook.remove()\n",
    "#         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metric calculations ####\n",
    "\n",
    "Entropy: Measures the uncertainty in the model's predictions\n",
    "\n",
    "Varentropy: Measures the variance of entropy across different positions\n",
    "\n",
    "Agreement: We measure how consistent the attention patterns are across different heads\n",
    "\n",
    "Interaction Strength: Measures the mean absolute attention values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize model with output_attentions=True\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "\n",
    "# Move model to the appropriate device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_and_varentropy(logits):\n",
    "    probs = F.softmax(logits, dim=-1) # get probabilities from logits using softmax\n",
    "\n",
    "    # entropy is the average of the negative sum of the probabilities times the log2 of the probabilities (measures the uncertainty of the model's predictions)\n",
    "    entropy = -torch.sum(probs * torch.log2(probs + 1e-10), dim=-1)\n",
    "\n",
    "    # varentropy is the variance of the entropy -- this is across different positions \n",
    "    varentropy = torch.var(entropy)\n",
    "    return entropy.mean(), varentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're computing the attention entropy and varentropy to effectively understand the model's focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_entropy_and_varentropy(attention_scores):\n",
    "    attention_entropies = []\n",
    "    for attn in attention_scores:\n",
    "        # attn shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_probs = attn  # This is already the attention probabilities\n",
    "        # Compute entropy over the source sequence (last dimension)\n",
    "        entropy = -torch.sum(attn_probs * torch.log2(attn_probs + 1e-10), dim=-1)  # Shape: (batch_size, num_heads, seq_len)\n",
    "        # Mean over batch and heads\n",
    "        entropy = entropy.mean(dim=(0, 1))  # Shape: (seq_len,)\n",
    "        # Mean over positions\n",
    "        entropy = entropy.mean()  # Scalar\n",
    "        attention_entropies.append(entropy)\n",
    "    \n",
    "    attention_entropies = torch.stack(attention_entropies)  # Shape: (num_layers,)\n",
    "    attention_entropy = attention_entropies.mean()\n",
    "    attention_varentropy = attention_entropies.var()\n",
    "    return attention_entropy, attention_varentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_agreement(attention_scores):\n",
    "    agreements = []\n",
    "    for attn in attention_scores:\n",
    "        # attn shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        mean_attention = attn.mean(dim=1, keepdim=True)  # Mean over heads\n",
    "        agreement = torch.abs(attn - mean_attention).mean(dim=(0, 1, 2, 3))  # Scalar\n",
    "        agreements.append(agreement)\n",
    "    attention_agreement = torch.stack(agreements).mean()\n",
    "    return attention_agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interaction_strength(attention_scores):\n",
    "    strengths = []\n",
    "    for attn in attention_scores:\n",
    "        # attn shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        strength = torch.abs(attn).mean(dim=(0, 1, 2, 3))  # Scalar\n",
    "        strengths.append(strength)\n",
    "    interaction_strength = torch.stack(strengths).mean()\n",
    "    return interaction_strength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    return next_token  # Shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARIFICATION_TOKEN = \"<clarify>\"\n",
    "\n",
    "def clarification_insertion(tokenizer, generated_tokens):\n",
    "    clarification_token_id = tokenizer.encode(CLARIFICATION_TOKEN, add_special_tokens=False)[0]\n",
    "    batch_size = generated_tokens.size(0)\n",
    "    clarification_token_ids = torch.full(\n",
    "        (batch_size, 1),\n",
    "        clarification_token_id,\n",
    "        device=generated_tokens.device,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    return clarification_token_ids  # Shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploration Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_sampling(logits, temperature=1.0, top_k=50):\n",
    "    logits = logits / temperature\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k=top_k)\n",
    "    probs = F.softmax(top_k_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)  # Shape: (batch_size, 1)\n",
    "    next_token = top_k_indices.gather(-1, next_token)\n",
    "    return next_token  # Shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_uncertainty_sampling(logits, temperature=1.5, top_p=0.9):\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)  # Shape: (batch_size, 1)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_sampling(logits, metrics, num_samples=12, base_temperature=1.0, base_top_p=0.9, base_top_k=50):\n",
    "    # Adjust parameters based on metrics\n",
    "    temperature = base_temperature * (1 + 0.3 * metrics['logits_uncertainty'] + 0.2 * metrics['attention_uncertainty'] - 0.2 * metrics['attention_agreement'])\n",
    "    temperature = max(0.5, min(temperature, 1.5))\n",
    "\n",
    "    top_p = base_top_p * (1 + 0.1 * metrics['attention_varentropy'])\n",
    "    top_p = max(0.1, min(top_p, 1.0))\n",
    "\n",
    "    top_k = int(base_top_k * (1 + 0.3 * metrics['interaction_strength'] - 0.2 * metrics['attention_agreement']))\n",
    "    top_k = max(5, min(top_k, logits.size(-1)))\n",
    "\n",
    "    # Generate multiple samples\n",
    "    logits = logits / temperature\n",
    "    next_tokens = []\n",
    "    scores = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        next_tokens.append(next_token)\n",
    "        log_prob = torch.log(probs.gather(-1, next_token))\n",
    "        confidence_score = calculate_confidence_score(metrics)\n",
    "        score = log_prob + confidence_score\n",
    "        scores.append(score)\n",
    "\n",
    "    # we want to select the best sample\n",
    "    best_index = torch.argmax(torch.stack(scores))\n",
    "    best_next_token = next_tokens[best_index]\n",
    "    return best_next_token\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=50, top_p=0.9, filter_value=-float('Inf')):\n",
    "    # top-k filtering - top-k filtering is a technique used to limit the number of tokens that can be selected by the model\n",
    "    if top_k > 0:\n",
    "        values_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits = logits.masked_fill(values_to_remove, filter_value)\n",
    "\n",
    "    # top-p filtering - top-p filtering is a technique used to limit the maximum probability of a token being selected\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        indices_to_remove = cumulative_probs > top_p\n",
    "        indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "        indices_to_remove[..., 0] = 0\n",
    "\n",
    "        sorted_indices_to_remove = indices_to_remove.scatter(1, sorted_indices, indices_to_remove)\n",
    "        logits = logits.masked_fill(sorted_indices_to_remove, filter_value)\n",
    "    return logits\n",
    "\n",
    "def calculate_confidence_score(metrics):\n",
    "    confidence_score = (\n",
    "        (1 - metrics[\"logits_entropy\"]) * 0.1 +\n",
    "        (1 - metrics[\"attention_entropy\"]) * 0.2 +\n",
    "        (1 - metrics[\"logits_varentropy\"]) * 0.3 +\n",
    "        (1 - metrics[\"attention_varentropy\"]) * 0.4 +\n",
    "        metrics[\"attention_agreement\"] * 0.5 +\n",
    "        metrics[\"interaction_strength\"] * 0.6\n",
    "    )\n",
    "    return confidence_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Parameter Adjustment\n",
    "We adjust sampling parameters based on the calculated metrics to dynamically adapt to the model's behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_parameters(metrics, base_params):\n",
    "    temperature = base_params['temperature'] * (1 + 0.3 * metrics['logits_uncertainty'] + 0.2 * metrics['attention_uncertainty'] - 0.2 * metrics['attention_agreement'])\n",
    "    temperature = max(0.5, min(temperature, 1.5))\n",
    "\n",
    "    top_p = base_params['top_p'] * (1 + 0.1 * metrics['attention_varentropy'])\n",
    "    top_p = max(0.1, min(top_p, 1.0))\n",
    "    top_k = int(base_params['top_k'] * (1 + 0.3 * metrics['interaction_strength'] - 0.2 * metrics['attention_agreement']))\n",
    "    top_k = max(5, min(top_k, 100))\n",
    "    min_p = base_params['min_p'] * (1 - 0.5 * metrics['logits_uncertainty'])\n",
    "    min_p = max(0.01, min(min_p, 0.5))\n",
    "    adjusted_params = {\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'top_k': top_k,\n",
    "        'min_p': min_p\n",
    "    }\n",
    "    return adjusted_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropix Sampling Function\n",
    "now just does the text generation using Entropix from the blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropix_generate(\n",
    "    model, tokenizer, input_text, max_length=50, base_params=None, num_samples=12\n",
    "):\n",
    "    if base_params is None:\n",
    "        base_params = {\n",
    "            'temperature': 1.0,\n",
    "            'top_p': 0.9,\n",
    "            'top_k': 50,\n",
    "            'min_p': 0.01\n",
    "        }\n",
    "    # Encode input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    \n",
    "    generated = input_ids\n",
    "    past_key_values = None\n",
    "\n",
    "    # Initialize lists to store metrics for plotting\n",
    "    metrics_history = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get model outputs\n",
    "        outputs = model(\n",
    "            input_ids=generated[:, -1:],  # Only pass the last token\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "\n",
    "        # Extract attention scores\n",
    "        attention_scores = outputs.attentions  # This is a tuple of attention matrices\n",
    "\n",
    "        # Calculate metrics\n",
    "        logits_entropy, logits_varentropy = calculate_entropy_and_varentropy(logits)\n",
    "        attention_entropy, attention_varentropy = calculate_attention_entropy_and_varentropy(attention_scores)\n",
    "        attention_agreement = calculate_attention_agreement(attention_scores)\n",
    "        interaction_strength = calculate_interaction_strength(attention_scores)\n",
    "\n",
    "        # Combine metrics into a dictionary\n",
    "        metrics = {\n",
    "            'logits_entropy': logits_entropy.item(),\n",
    "            'logits_varentropy': logits_varentropy.item(),\n",
    "            'attention_entropy': attention_entropy.item(),\n",
    "            'attention_varentropy': attention_varentropy.item(),\n",
    "            'attention_agreement': attention_agreement.item(),\n",
    "            'interaction_strength': interaction_strength.item(),\n",
    "            'logits_uncertainty': logits_entropy.item() + logits_varentropy.item(),\n",
    "            'attention_uncertainty': attention_entropy.item() + attention_varentropy.item(),\n",
    "        }\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        metrics_history.append(metrics.copy())\n",
    "\n",
    "        # Decide on sampling strategy\n",
    "        if logits_entropy.item() < 0.1 and logits_varentropy.item() < 0.1:\n",
    "            # Greedy sampling\n",
    "            next_token = greedy_sampling(logits)\n",
    "        elif logits_entropy.item() > 3.0 and logits_varentropy.item() < 0.1:\n",
    "            # Clarification insertion\n",
    "            next_token = clarification_insertion(tokenizer, generated)\n",
    "        elif logits_entropy.item() < 2.0 and logits_varentropy.item() > 5.0:\n",
    "            # Exploration sampling\n",
    "            adjusted_params = adjust_parameters(metrics, base_params)\n",
    "            next_token = exploration_sampling(\n",
    "                logits,\n",
    "                temperature=adjusted_params['temperature'],\n",
    "                top_k=adjusted_params['top_k']\n",
    "            )\n",
    "        elif logits_entropy.item() > 5.0 and logits_varentropy.item() > 5.0:\n",
    "            # High uncertainty sampling\n",
    "            adjusted_params = adjust_parameters(metrics, base_params)\n",
    "            next_token = high_uncertainty_sampling(\n",
    "                logits,\n",
    "                temperature=adjusted_params['temperature'],\n",
    "                top_p=adjusted_params['top_p']\n",
    "            )\n",
    "        else:\n",
    "            # Adaptive sampling\n",
    "            adjusted_params = adjust_parameters(metrics, base_params)\n",
    "            next_token = adaptive_sampling(\n",
    "                logits,\n",
    "                metrics,\n",
    "                num_samples=num_samples,\n",
    "                base_temperature=adjusted_params['temperature'],\n",
    "                base_top_p=adjusted_params['top_p'],\n",
    "                base_top_k=adjusted_params['top_k']\n",
    "            )\n",
    "\n",
    "        print(f\"generated shape before: {generated.shape}\")\n",
    "        print(f\"next_token shape before unsqueeze: {next_token.shape}\")\n",
    "\n",
    "        # next_token = next_token.unsqueeze(-1)\n",
    "\n",
    "        print(f\"next_token shape after unsqueeze: {next_token.shape}\")\n",
    "\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "        print(f\"generated shape after: {generated.shape}\")\n",
    "        # Check for end-of-sentence token\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    # Extract the generated continuation (excluding the input text)\n",
    "    continuation_ids = generated[:, input_ids.size(1):]\n",
    "\n",
    "    # Decode the continuation\n",
    "    continuation_text = tokenizer.decode(continuation_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "    # Return both the continuation text and metrics history\n",
    "    return continuation_text, metrics_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Low Uncertainty Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated shape before: torch.Size([1, 8])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 9])\n",
      "generated shape before: torch.Size([1, 9])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 10])\n",
      "generated shape before: torch.Size([1, 10])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 11])\n",
      "generated shape before: torch.Size([1, 11])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 12])\n",
      "generated shape before: torch.Size([1, 12])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 13])\n",
      "generated shape before: torch.Size([1, 13])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 14])\n",
      "generated shape before: torch.Size([1, 14])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 15])\n",
      "generated shape before: torch.Size([1, 15])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 16])\n",
      "generated shape before: torch.Size([1, 16])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 17])\n",
      "generated shape before: torch.Size([1, 17])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 18])\n",
      "Generated continuation: \n",
      "\n",
      "<p>This is a very simple\n"
     ]
    }
   ],
   "source": [
    "input_text = \"the place to visit in MA is \"\n",
    "continuation_text, metrics_history = entropix_generate(model, tokenizer, input_text, max_length=10)\n",
    "print(\"Generated continuation:\", continuation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated shape before: torch.Size([1, 8])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 9])\n",
      "generated shape before: torch.Size([1, 9])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 10])\n",
      "generated shape before: torch.Size([1, 10])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 11])\n",
      "generated shape before: torch.Size([1, 11])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 12])\n",
      "generated shape before: torch.Size([1, 12])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 13])\n",
      "generated shape before: torch.Size([1, 13])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 14])\n",
      "generated shape before: torch.Size([1, 14])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 15])\n",
      "generated shape before: torch.Size([1, 15])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 16])\n",
      "generated shape before: torch.Size([1, 16])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 17])\n",
      "generated shape before: torch.Size([1, 17])\n",
      "next_token shape before unsqueeze: torch.Size([1, 1])\n",
      "next_token shape after unsqueeze: torch.Size([1, 1])\n",
      "generated shape after: torch.Size([1, 18])\n",
      "('the place to visit in MA is \\n\\nThe following is a list of all the', [{'logits_entropy': 11.475199699401855, 'logits_varentropy': nan, 'attention_entropy': 0.0, 'attention_varentropy': 0.0, 'attention_agreement': 0.0, 'interaction_strength': 1.0, 'logits_uncertainty': nan, 'attention_uncertainty': 0.0}, {'logits_entropy': 0.13565953075885773, 'logits_varentropy': nan, 'attention_entropy': 0.287627249956131, 'attention_varentropy': 0.011293912306427956, 'attention_agreement': 0.08966539055109024, 'interaction_strength': 0.5, 'logits_uncertainty': nan, 'attention_uncertainty': 0.29892116226255894}, {'logits_entropy': 10.643041610717773, 'logits_varentropy': nan, 'attention_entropy': 0.7490877509117126, 'attention_varentropy': 0.05072861537337303, 'attention_agreement': 0.09075699001550674, 'interaction_strength': 0.333333283662796, 'logits_uncertainty': nan, 'attention_uncertainty': 0.7998163662850857}, {'logits_entropy': 12.776665687561035, 'logits_varentropy': nan, 'attention_entropy': 0.9898346066474915, 'attention_varentropy': 0.12241361290216446, 'attention_agreement': 0.08292517811059952, 'interaction_strength': 0.25, 'logits_uncertainty': nan, 'attention_uncertainty': 1.112248219549656}, {'logits_entropy': 9.241388320922852, 'logits_varentropy': nan, 'attention_entropy': 1.1439416408538818, 'attention_varentropy': 0.17749223113059998, 'attention_agreement': 0.07186134904623032, 'interaction_strength': 0.20000003278255463, 'logits_uncertainty': nan, 'attention_uncertainty': 1.3214338719844818}, {'logits_entropy': 3.9595537185668945, 'logits_varentropy': nan, 'attention_entropy': 1.3108447790145874, 'attention_varentropy': 0.20680291950702667, 'attention_agreement': 0.07205881178379059, 'interaction_strength': 0.1666666716337204, 'logits_uncertainty': nan, 'attention_uncertainty': 1.517647698521614}, {'logits_entropy': 7.780191898345947, 'logits_varentropy': nan, 'attention_entropy': 1.441382884979248, 'attention_varentropy': 0.2663746774196625, 'attention_agreement': 0.06542511284351349, 'interaction_strength': 0.1428571343421936, 'logits_uncertainty': nan, 'attention_uncertainty': 1.7077575623989105}, {'logits_entropy': 0.3650969862937927, 'logits_varentropy': nan, 'attention_entropy': 1.4297338724136353, 'attention_varentropy': 0.363251268863678, 'attention_agreement': 0.0584760457277298, 'interaction_strength': 0.125, 'logits_uncertainty': nan, 'attention_uncertainty': 1.7929851412773132}, {'logits_entropy': 9.343316078186035, 'logits_varentropy': nan, 'attention_entropy': 1.6778043508529663, 'attention_varentropy': 0.3270980715751648, 'attention_agreement': 0.054918963462114334, 'interaction_strength': 0.1111111044883728, 'logits_uncertainty': nan, 'attention_uncertainty': 2.004902422428131}, {'logits_entropy': 9.099605560302734, 'logits_varentropy': nan, 'attention_entropy': 1.700927734375, 'attention_varentropy': 0.38458532094955444, 'attention_agreement': 0.04910092055797577, 'interaction_strength': 0.10000001639127731, 'logits_uncertainty': nan, 'attention_uncertainty': 2.0855130553245544}])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"the place to visit in MA is \"\n",
    "output_text = entropix_generate(model, tokenizer, input_text, max_length=10)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_metrics(metrics_history):\n",
    "    logits_entropy = [m['logits_entropy'] for m in metrics_history]\n",
    "    logits_varentropy = [m['logits_varentropy'] for m in metrics_history]\n",
    "    attention_entropy = [m['attention_entropy'] for m in metrics_history]\n",
    "    attention_varentropy = [m['attention_varentropy'] for m in metrics_history]\n",
    "    attention_agreement = [m['attention_agreement'] for m in metrics_history]\n",
    "    interaction_strength = [m['interaction_strength'] for m in metrics_history]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(logits_entropy)\n",
    "    plt.title('Logits Entropy')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(logits_varentropy)\n",
    "    plt.title('Logits Varentropy')\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(attention_entropy)\n",
    "    plt.title('Attention Entropy')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(attention_varentropy)\n",
    "    plt.title('Attention Varentropy')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(attention_agreement)\n",
    "    plt.title('Attention Agreement')\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(interaction_strength)\n",
    "    plt.title('Interaction Strength')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m output_text, metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mentropix_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n\u001b[1;32m      5\u001b[0m plot_metrics(metrics_history)\n",
      "Cell \u001b[0;32mIn[122], line 93\u001b[0m, in \u001b[0;36mentropix_generate\u001b[0;34m(model, tokenizer, input_text, max_length, base_params, num_samples)\u001b[0m\n\u001b[1;32m     83\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m adaptive_sampling(\n\u001b[1;32m     84\u001b[0m         logits,\n\u001b[1;32m     85\u001b[0m         metrics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m         base_top_k\u001b[38;5;241m=\u001b[39madjusted_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Append generated token\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Check for end-of-sentence token\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_token\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "input_text = \"The capital of France is\"\n",
    "output_text, metrics_history = entropix_generate(model, tokenizer, input_text, max_length=10)\n",
    "print(output_text)\n",
    "\n",
    "plot_metrics(metrics_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a distant future, humanity has colonized Mars and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m output_text, metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mentropix_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n",
      "Cell \u001b[0;32mIn[122], line 93\u001b[0m, in \u001b[0;36mentropix_generate\u001b[0;34m(model, tokenizer, input_text, max_length, base_params, num_samples)\u001b[0m\n\u001b[1;32m     83\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m adaptive_sampling(\n\u001b[1;32m     84\u001b[0m         logits,\n\u001b[1;32m     85\u001b[0m         metrics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m         base_top_k\u001b[38;5;241m=\u001b[39madjusted_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Append generated token\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Check for end-of-sentence token\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_token\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "input_text = \"In a distant future, humanity has colonized Mars and\"\n",
    "output_text, metrics_history = entropix_generate(model, tokenizer, input_text, max_length=10)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
