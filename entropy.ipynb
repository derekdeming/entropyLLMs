{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: filelock in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from transformers==4.31.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.11.17)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.31.0\n",
      "Requirement already satisfied: torch in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.31.0\n",
    "!pip install torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/derekdeming/.pyenv/versions/3.10.8/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT 2 WITH ATTENTION OUTPUTS ####\n",
    "\n",
    "with this we're registering forward hooks on each attention layer to capture the attention outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2WithAttentionOutputs(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.attention_scores = []\n",
    "\n",
    "    def reset_attention_scores(self):\n",
    "        self.attention_scores = []\n",
    "\n",
    "    def save_attention_scores(self, module, input, output):\n",
    "        self.attention_scores.append(output[1]) # output[1] contains the attention probabilities\n",
    "\n",
    "    def forward(self, input_ids, past_key_values=None, attention_mask=None):\n",
    "        self.reset_attention_scores()\n",
    "\n",
    "        # use hooks to capture attention outputs\n",
    "        hooks = []\n",
    "        for block in self.transformer.h:\n",
    "            hook = block.attn.register_forward_hook(self.save_attention_scores)\n",
    "            hooks.append(hook)\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 7.03MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metric calculations ####\n",
    "\n",
    "Entropy: Measures the uncertainty in the model's predictions\n",
    "\n",
    "Varentropy: Measures the variance of entropy across different positions\n",
    "\n",
    "Agreement: We measure how consistent the attention patterns are across different heads\n",
    "\n",
    "Interaction Strength: Measures the mean absolute attention values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize model with output_attentions=True\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "\n",
    "# Move model to the appropriate device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_and_varentropy(logits):\n",
    "    probs = F.softmax(logits, dim=-1) # get probabilities from logits using softmax\n",
    "\n",
    "    # entropy is the average of the negative sum of the probabilities times the log2 of the probabilities (measures the uncertainty of the model's predictions)\n",
    "    entropy = -torch.sum(probs * torch.log2(probs + 1e-10), dim=-1)\n",
    "\n",
    "    # varentropy is the variance of the entropy -- this is across different positions \n",
    "    varentropy = torch.var(entropy)\n",
    "    return entropy.mean(), varentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're computing the attention entropy and varentropy to effectively understand the model's focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_entropy_and_varentropy(attention_scores):\n",
    "    attention_entropies = []\n",
    "    for attn in attention_scores:\n",
    "        # attn shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_probs = attn  # This is already the attention probabilities\n",
    "        # Compute entropy over the source sequence (last dimension)\n",
    "        entropy = -torch.sum(attn_probs * torch.log2(attn_probs + 1e-10), dim=-1)  # Shape: (batch_size, num_heads, seq_len)\n",
    "        # Mean over batch and heads\n",
    "        entropy = entropy.mean(dim=(0, 1))  # Shape: (seq_len,)\n",
    "        # Mean over positions\n",
    "        entropy = entropy.mean()  # Scalar\n",
    "        attention_entropies.append(entropy)\n",
    "    \n",
    "    attention_entropies = torch.stack(attention_entropies)  # Shape: (num_layers,)\n",
    "    attention_entropy = attention_entropies.mean()\n",
    "    attention_varentropy = attention_entropies.var()\n",
    "    return attention_entropy, attention_varentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention_agreement(attention_scores):\n",
    "    agreements = []\n",
    "    for attn in attention_scores:\n",
    "        # attn shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        mean_attention = attn.mean(dim=1, keepdim=True)  # Mean over heads\n",
    "        agreement = torch.abs(attn - mean_attention).mean(dim=(0, 1, 2, 3))  # Scalar\n",
    "        agreements.append(agreement)\n",
    "    attention_agreement = torch.stack(agreements).mean()\n",
    "    return attention_agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interaction_strength(attention_scores):\n",
    "    strengths = []\n",
    "    for attn in attention_scores:\n",
    "        # attn shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        strength = torch.abs(attn).mean(dim=(0, 1, 2, 3))  # Scalar\n",
    "        strengths.append(strength)\n",
    "    interaction_strength = torch.stack(strengths).mean()\n",
    "    return interaction_strength\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "    return next_token  # Shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARIFICATION_TOKEN = \"<clarify>\"\n",
    "\n",
    "def clarification_insertion(tokenizer, generated_tokens):\n",
    "    clarification_token_id = tokenizer.encode(CLARIFICATION_TOKEN, add_special_tokens=False)[0]\n",
    "    batch_size = generated_tokens.size(0)\n",
    "    clarification_token_ids = torch.full(\n",
    "        (batch_size, 1),\n",
    "        clarification_token_id,\n",
    "        device=generated_tokens.device,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    return clarification_token_ids  # Shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploration Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_sampling(logits, temperature=1.0, top_k=50):\n",
    "    logits = logits / temperature\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k=top_k)\n",
    "    probs = F.softmax(top_k_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)  # Shape: (batch_size, 1)\n",
    "    next_token = top_k_indices.gather(-1, next_token)\n",
    "    return next_token  # Shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_uncertainty_sampling(logits, temperature=1.5, top_p=0.9):\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)  # Shape: (batch_size, 1)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_sampling(logits, metrics, num_samples=12, base_temperature=1.0, base_top_p=0.9, base_top_k=50):\n",
    "    # Adjust parameters based on metrics\n",
    "    temperature = base_temperature * (1 + 0.3 * metrics['logits_uncertainty'] + 0.2 * metrics['attention_uncertainty'] - 0.2 * metrics['attention_agreement'])\n",
    "    temperature = max(0.5, min(temperature, 1.5))\n",
    "\n",
    "    top_p = base_top_p * (1 + 0.1 * metrics['attention_varentropy'])\n",
    "    top_p = max(0.1, min(top_p, 1.0))\n",
    "\n",
    "    top_k = int(base_top_k * (1 + 0.3 * metrics['interaction_strength'] - 0.2 * metrics['attention_agreement']))\n",
    "    top_k = max(5, min(top_k, logits.size(-1)))\n",
    "\n",
    "    # Generate multiple samples\n",
    "    logits = logits / temperature\n",
    "    next_tokens = []\n",
    "    scores = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        next_tokens.append(next_token)\n",
    "        log_prob = torch.log(probs.gather(-1, next_token))\n",
    "        confidence_score = calculate_confidence_score(metrics)\n",
    "        score = log_prob + confidence_score\n",
    "        scores.append(score)\n",
    "\n",
    "    # we want to select the best sample\n",
    "    best_index = torch.argmax(torch.stack(scores))\n",
    "    best_next_token = next_tokens[best_index]\n",
    "    return best_next_token\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=50, top_p=0.9, filter_value=-float('Inf')):\n",
    "    # top-k filtering - top-k filtering is a technique used to limit the number of tokens that can be selected by the model\n",
    "    if top_k > 0:\n",
    "        values_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits = logits.masked_fill(values_to_remove, filter_value)\n",
    "\n",
    "    # top-p filtering - top-p filtering is a technique used to limit the maximum probability of a token being selected\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        indices_to_remove = cumulative_probs > top_p\n",
    "        indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "        indices_to_remove[..., 0] = 0\n",
    "\n",
    "        sorted_indices_to_remove = indices_to_remove.scatter(1, sorted_indices, indices_to_remove)\n",
    "        logits = logits.masked_fill(sorted_indices_to_remove, filter_value)\n",
    "    return logits\n",
    "\n",
    "def calculate_confidence_score(metrics):\n",
    "    confidence_score = (\n",
    "        (1 - metrics[\"logits_entropy\"]) * 0.1 +\n",
    "        (1 - metrics[\"attention_entropy\"]) * 0.2 +\n",
    "        (1 - metrics[\"logits_varentropy\"]) * 0.3 +\n",
    "        (1 - metrics[\"attention_varentropy\"]) * 0.4 +\n",
    "        metrics[\"attention_agreement\"] * 0.5 +\n",
    "        metrics[\"interaction_strength\"] * 0.6\n",
    "    )\n",
    "    return confidence_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Parameter Adjustment\n",
    "We adjust sampling parameters based on the calculated metrics to dynamically adapt to the model's behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_parameters(metrics, base_params):\n",
    "    temperature = base_params['temperature'] * (1 + 0.3 * metrics['logits_uncertainty'] + 0.2 * metrics['attention_uncertainty'] - 0.2 * metrics['attention_agreement'])\n",
    "    temperature = max(0.5, min(temperature, 1.5))\n",
    "\n",
    "    top_p = base_params['top_p'] * (1 + 0.1 * metrics['attention_varentropy'])\n",
    "    top_p = max(0.1, min(top_p, 1.0))\n",
    "    top_k = int(base_params['top_k'] * (1 + 0.3 * metrics['interaction_strength'] - 0.2 * metrics['attention_agreement']))\n",
    "    top_k = max(5, min(top_k, 100))\n",
    "    min_p = base_params['min_p'] * (1 - 0.5 * metrics['logits_uncertainty'])\n",
    "    min_p = max(0.01, min(min_p, 0.5))\n",
    "    adjusted_params = {\n",
    "        'temperature': temperature,\n",
    "        'top_p': top_p,\n",
    "        'top_k': top_k,\n",
    "        'min_p': min_p\n",
    "    }\n",
    "    return adjusted_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropix Sampling Function\n",
    "now just does the text generation using Entropix from the blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropix_generate(\n",
    "    model, tokenizer, input_text, max_length=50, base_params=None, num_samples=12\n",
    "):\n",
    "    if base_params is None:\n",
    "        base_params = {\n",
    "            'temperature': 1.0,\n",
    "            'top_p': 0.9,\n",
    "            'top_k': 50,\n",
    "            'min_p': 0.01\n",
    "        }\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_ids = input_ids.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    generated = input_ids  # Shape: (batch_size, seq_len)\n",
    "    past_key_values = None\n",
    "\n",
    "    # Initialize a list to store metrics history (optional)\n",
    "    metrics_history = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Model outputs with attentions\n",
    "        outputs = model(\n",
    "            input_ids=generated,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        logits = outputs.logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Get attention scores from outputs.attentions\n",
    "        attention_scores = outputs.attentions  # Tuple of (num_layers) tensors\n",
    "\n",
    "        # Compute metrics\n",
    "        logits_entropy, logits_varentropy = calculate_entropy_and_varentropy(logits)\n",
    "        attention_entropy, attention_varentropy = calculate_attention_entropy_and_varentropy(attention_scores)\n",
    "        attention_agreement = calculate_attention_agreement(attention_scores)\n",
    "        interaction_strength = calculate_interaction_strength(attention_scores)\n",
    "        metrics = {\n",
    "            'logits_entropy': logits_entropy.item(),\n",
    "            'logits_varentropy': logits_varentropy.item(),\n",
    "            'attention_entropy': attention_entropy.item(),\n",
    "            'attention_varentropy': attention_varentropy.item(),\n",
    "            'attention_agreement': attention_agreement.item(),\n",
    "            'interaction_strength': interaction_strength.item(),\n",
    "            'logits_uncertainty': logits_entropy.item() + logits_varentropy.item(),\n",
    "            'attention_uncertainty': attention_entropy.item() + attention_varentropy.item(),\n",
    "        }\n",
    "\n",
    "        # Store metrics (optional, for plotting or analysis)\n",
    "        metrics_history.append(metrics)\n",
    "\n",
    "        # Sampling strategy based on metrics\n",
    "        if logits_entropy.item() < 0.1 and logits_varentropy.item() < 0.1:\n",
    "            next_token = greedy_sampling(logits)\n",
    "        elif logits_entropy.item() > 3.0 and logits_varentropy.item() < 0.1:\n",
    "            next_token = clarification_insertion(tokenizer, generated)\n",
    "        elif logits_entropy.item() < 2.0 and logits_varentropy.item() > 5.0:\n",
    "            adjusted_params = adjust_parameters(metrics, base_params)\n",
    "            next_token = exploration_sampling(\n",
    "                logits,\n",
    "                temperature=adjusted_params['temperature'],\n",
    "                top_k=adjusted_params['top_k']\n",
    "            )\n",
    "        elif logits_entropy.item() > 5.0 and logits_varentropy.item() > 5.0:\n",
    "            # High uncertainty sampling\n",
    "            adjusted_params = adjust_parameters(metrics, base_params)\n",
    "            next_token = high_uncertainty_sampling(\n",
    "                logits,\n",
    "                temperature=adjusted_params['temperature'],\n",
    "                top_p=adjusted_params['top_p']\n",
    "            )\n",
    "        else:\n",
    "            # Adaptive sampling\n",
    "            adjusted_params = adjust_parameters(metrics, base_params)\n",
    "            next_token = adaptive_sampling(\n",
    "                logits,\n",
    "                metrics,\n",
    "                num_samples=num_samples,\n",
    "                base_temperature=adjusted_params['temperature'],\n",
    "                base_top_p=adjusted_params['top_p'],\n",
    "                base_top_k=adjusted_params['top_k']\n",
    "            )\n",
    "\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    output_text = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Low Uncertainty Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the place to visit in MA is  the place to visit in MA is  \n"
     ]
    }
   ],
   "source": [
    "input_text = \"the place to visit in MA is \"\n",
    "output_text = entropix_generate(model, tokenizer, input_text, max_length=10)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_metrics(metrics_history):\n",
    "    logits_entropy = [m['logits_entropy'] for m in metrics_history]\n",
    "    logits_varentropy = [m['logits_varentropy'] for m in metrics_history]\n",
    "    attention_entropy = [m['attention_entropy'] for m in metrics_history]\n",
    "    attention_varentropy = [m['attention_varentropy'] for m in metrics_history]\n",
    "    attention_agreement = [m['attention_agreement'] for m in metrics_history]\n",
    "    interaction_strength = [m['interaction_strength'] for m in metrics_history]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(logits_entropy)\n",
    "    plt.title('Logits Entropy')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(logits_varentropy)\n",
    "    plt.title('Logits Varentropy')\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(attention_entropy)\n",
    "    plt.title('Attention Entropy')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(attention_varentropy)\n",
    "    plt.title('Attention Varentropy')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(attention_agreement)\n",
    "    plt.title('Attention Agreement')\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(interaction_strength)\n",
    "    plt.title('Interaction Strength')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a distant future, humanity has colonized Mars and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m output_text, metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mentropix_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n",
      "Cell \u001b[0;32mIn[94], line 24\u001b[0m, in \u001b[0;36mentropix_generate\u001b[0;34m(model, tokenizer, input_text, max_length, base_params, num_samples)\u001b[0m\n\u001b[1;32m     20\u001b[0m metrics_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Model outputs with attentions\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Shape: (batch_size, vocab_size)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:844\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[0;32m--> 844\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "input_text = \"In a distant future, humanity has colonized Mars and\"\n",
    "output_text, metrics_history = entropix_generate(model, tokenizer, input_text, max_length=50)\n",
    "print(output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2WithAttentionOutputs(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize your custom model that captures attention outputs\n",
    "model = GPT2WithAttentionOutputs.from_pretrained('gpt2', output_attentions=True)\n",
    "\n",
    "# Move model to appropriate device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
